<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Machine Learning - Software Engineering</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../../favicon.svg">
        <link rel="shortcut icon" href="../../favicon.png">
        <link rel="stylesheet" href="../../css/variables.css">
        <link rel="stylesheet" href="../../css/general.css">
        <link rel="stylesheet" href="../../css/chrome.css">
        <link rel="stylesheet" href="../../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../../highlight.css">
        <link rel="stylesheet" href="../../tomorrow-night.css">
        <link rel="stylesheet" href="../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../../index.html">Software Engineering</a></li><li class="spacer"></li><li class="chapter-item expanded affix "><li class="part-title">Chapters</li><li class="chapter-item expanded "><a href="../../chapters/DevOps/index.html"><strong aria-hidden="true">1.</strong> DevOps</a></li><li class="chapter-item expanded "><a href="../../chapters/Flutter/index.html"><strong aria-hidden="true">2.</strong> Flutter</a></li><li class="chapter-item expanded "><a href="../../chapters/Infrastructure_As_Code/index.html"><strong aria-hidden="true">3.</strong> Infrastructure As Code</a></li><li class="chapter-item expanded "><a href="../../chapters/Machine_Learning/index.html" class="active"><strong aria-hidden="true">4.</strong> Machine Learning</a></li><li class="chapter-item expanded "><a href="../../chapters/Software_Architecture/index.html"><strong aria-hidden="true">5.</strong> Software Architecture</a></li><li class="chapter-item expanded "><a href="../../chapters/Software_Testing/index.html"><strong aria-hidden="true">6.</strong> Software Testing</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Software Engineering</h1>

                    <div class="right-buttons">
                        <a href="../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="machine-learning"><a class="header" href="#machine-learning">Machine Learning</a></h1>
<hr />
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="#numpy">Numpy</a>
<ul>
<li><a href="#what-is-numpy-array">What Is Numpy Array?</a></li>
<li><a href="#list-to-numpy">List To Numpy</a></li>
<li><a href="#numpy-indexing-and-slicing">NumPy Indexing and Slicing</a></li>
<li><a href="#filtering">Filtering</a></li>
</ul>
</li>
<li><a href="#pandas">Pandas</a>
<ul>
<li><a href="#data-types">Data Types</a></li>
<li><a href="#loc---label-based-indexing">‘loc’ - Label-Based Indexing</a></li>
<li><a href="#identifying-missing-data">Identifying Missing Data</a></li>
<li><a href="#filling-missing-data">Filling Missing Data</a></li>
<li><a href="#groupby">GroupBy</a></li>
</ul>
</li>
<li><a href="#matplotlib">Matplotlib</a>
<ul>
<li><a href="#line-plot">Line Plot</a></li>
<li><a href="#scatter-plot">Scatter Plot</a></li>
<li><a href="#bar-plot">Bar Plot</a></li>
</ul>
</li>
<li><a href="#data-processing">Data Processing</a>
<ul>
<li><a href="#missing-values">Missing Values</a></li>
<li><a href="#errors-and-noise">Errors and Noise</a></li>
</ul>
</li>
<li><a href="#model-validation">Model Validation</a>
<ul>
<li><a href="#training-and-test-sets">Training and Test Sets</a></li>
<li><a href="#cross-validation">Cross-Validation</a></li>
</ul>
</li>
<li><a href="#model-selection">Model Selection</a>
<ul>
<li><a href="#bias-and-variance">Bias and Variance</a></li>
</ul>
</li>
<li><a href="#supervised-learning">Supervised Learning</a>
<ul>
<li><a href="#regression-and-classification-models">Regression and Classification Models</a></li>
</ul>
</li>
<li><a href="#linear-models">Linear Models</a></li>
<li><a href="#linear-models-for-regression">Linear Models For Regression</a>
<ul>
<li><a href="#linear-regression">Linear Regression</a></li>
<li><a href="#ridge-regression">Ridge Regression</a></li>
<li><a href="#lasso-regression">Lasso Regression</a></li>
</ul>
</li>
<li><a href="#linear-regression-accuracy-metrics">Linear Regression Accuracy Metrics</a>
<ul>
<li><a href="#mean-squared-error-mse">Mean Squared Error (MSE)</a></li>
<li><a href="#r%C2%B2-score-coefficient-of-determination">R² Score (Coefficient of Determination)</a></li>
</ul>
</li>
<li><a href="#linear-models-for-classification">Linear Models For Classification</a>
<ul>
<li><a href="#logistic-regression">Logistic Regression</a></li>
</ul>
</li>
<li><a href="#linear-classification-accuracy-metrics">Linear Classification Accuracy Metrics</a>
<ul>
<li><a href="#confusion-matrix">Confusion Matrix</a></li>
<li><a href="#accuracy">Accuracy</a></li>
<li><a href="#precision">Precision</a></li>
<li><a href="#recall">Recall</a></li>
<li><a href="#f1-score">F1 Score</a></li>
</ul>
</li>
<li><a href="#decision-trees">Decision Trees</a>
<ul>
<li><a href="#key-concepts">Key Concepts</a></li>
<li><a href="#gini-impurity">Gini Impurity</a></li>
<li><a href="#decision-tree-classification-uses-gini-impurity">Decision Tree Classification Uses Gini Impurity</a></li>
<li><a href="#predicting-new-values">Predicting New Values</a></li>
<li><a href="#difference-between-hyperparameter-and-parameter">Difference Between Hyperparameter and Parameter</a></li>
<li><a href="#decision-tree-hyperparameter">Decision Tree Hyperparameter</a></li>
<li><a href="#decision-tree-regression">Decision Tree Regression</a></li>
<li><a href="#decision-tree-strengths">Decision Tree Strengths</a></li>
<li><a href="#decision-tree-weakness">Decision Tree Weakness</a></li>
</ul>
</li>
<li><a href="#random-forest">Random Forest</a>
<ul>
<li><a href="#steps-to-create-a-random-forest">Steps to Create a Random Forest</a></li>
</ul>
</li>
</ul>
<hr />
<h2 id="numpy"><a class="header" href="#numpy">Numpy</a></h2>
<p><img src="./images/image36.png" alt="Numpy" /></p>
<p>Python includes a module named numpy that can be used to store data in a matrix-like object.</p>
<p>Import statement:</p>
<pre><code class="language-python">import numpy as np
</code></pre>
<h3 id="what-is-numpy-array"><a class="header" href="#what-is-numpy-array">What Is Numpy Array?</a></h3>
<ul>
<li>A multi-dimensional array (data type = ndarray) can be created from a multi-dimensional list using the NumPy module.</li>
<li>A one-dimensional array is an array that has only one dimension and contains elements of the same type and size.</li>
</ul>
<p><img src="./images/image8.png" alt="Numpy Array" /></p>
<ul>
<li>A two-dimensional array is an array that has two dimensions and contains elements of the same type and size.</li>
</ul>
<p><img src="./images/image9.png" alt="Numpy Array" /></p>
<ul>
<li>An array has “axis/axes” to indicate its dimensions.</li>
<li>The first axis (axis = 0) of a 2-D array shows the number of rows and the second axis (axis = 1) shows the number of columns.</li>
<li>Indexing or slicing the array can be used to get or change its elements, similar to lists.</li>
</ul>
<h3 id="list-to-numpy"><a class="header" href="#list-to-numpy">List To Numpy</a></h3>
<pre><code class="language-python">list1 = [1, 2, 3, 4, 5]
arr1 = npm.array(list1)
</code></pre>
<p>This also works for multi-dimensional lists, but only if the list elements have the same type (a list with both integers and floats will be converted to all floats)</p>
<h3 id="numpy-indexing-and-slicing"><a class="header" href="#numpy-indexing-and-slicing">NumPy Indexing and Slicing</a></h3>
<ul>
<li>You can index NumPy arrays similar to lists</li>
<li>You can also slice NumPy arrays like lists</li>
</ul>
<p>For a 2-D array:</p>
<pre><code class="language-python">sub_arr = arr[start_row:end_row, start_col:end_col]
</code></pre>
<p>To access a whole row or column you can use empty slicing:</p>
<ul>
<li><code>arr[:,0]</code> for all the rows in the first column</li>
<li><code>arr[0,:]</code> for all the columns in the first row</li>
</ul>
<p>Example:</p>
<pre><code class="language-python">arr = np.array([1,2,3,4,5,6])

print(arr[3]) #Output: 4

test1 = arr[3:]
test1[0] = 99

print(arr) # Output: [99, 5, 6]
</code></pre>
<h3 id="filtering"><a class="header" href="#filtering">Filtering</a></h3>
<p>To select array elements that meet a certain criterion, you can apply a <code>conditional expression</code>.</p>
<p>Example:</p>
<pre><code class="language-python"># Creating a NumPy array with elements -2, -1, 0, 1, and 2
arr = np.array([-2, -1, 0, 1, 2])

# Selecting and displaying only the elements in the array that are greater than 0
selected_elements = arr[arr &gt; 0] # Output: array([1, 2])
</code></pre>
<p>We could also use <code>np.where()</code>.</p>
<p>Example:</p>
<pre><code class="language-python"># Creating a NumPy array with elements -2, -1, 0, 1, and 2
arr = np.array([-2, -1, 0, 1, 2])

# Using np.where to find the indices where the elements are greater than 0
indices = np.where(arr &gt; 0)

# Selecting and displaying the elements that satisfy the condition using the obtained indices
selected_elements = arr[indices]
</code></pre>
<hr />
<h2 id="pandas"><a class="header" href="#pandas">Pandas</a></h2>
<p><img src="./images/image37.png" alt="Pandas" /></p>
<p>Pandas is a Python module used to import, export, and manipulate data.</p>
<p>import statement:</p>
<pre><code class="language-python">import pandas as pd
</code></pre>
<h3 id="data-types"><a class="header" href="#data-types">Data Types</a></h3>
<p>When data is imported using pandas, there are two different data types depending on the dimensions:</p>
<ul>
<li>1-D data is stored in a Series</li>
<li>2-D data is stored in a DataFrame</li>
<li>Each column in a DataFrame represents a Series</li>
<li>The values in each Series (data frame columns) must be the same type.</li>
</ul>
<h3 id="loc---label-based-indexing"><a class="header" href="#loc---label-based-indexing">‘loc’ - Label-Based Indexing</a></h3>
<p>The <code>loc</code> method in Pandas lets you access DataFrame data by labels or boolean array-based indexing.
Likewise, the <code>iloc</code> method lets you access DataFrame data by integer positions, like indexing elements in a Python list.</p>
<p>Example:</p>
<pre><code class="language-python"># Selecting two rows and all columns that have the index values 'ID1' and 'ID3'
df.loc[['ID1', 'ID3'], :]

# Selecting multiple rows and columns where age is greater than 30 and then selecting the 'Name' and 'Age' columns
df.loc[df['Age'] &gt; 30, ['Name', 'Age']]
</code></pre>
<p><img src="./images/image7.png" alt="loc" /></p>
<h3 id="identifying-missing-data"><a class="header" href="#identifying-missing-data">Identifying Missing Data</a></h3>
<p>The <code>isna()</code> and <code>isnull()</code> methods are used interchangeably to check for missing values within a DataFrame or Series.</p>
<pre><code class="language-python">df.isna()

# or

df.isnull()
</code></pre>
<p><img src="./images/image1.png" alt="isna() vs isnull()" /></p>
<h3 id="filling-missing-data"><a class="header" href="#filling-missing-data">Filling Missing Data</a></h3>
<p>The <code>fillna()</code> function is a versatile tool for replacing missing or NaN (Not a Number) values within a DataFrame or Series. Available methods are <code>ffill</code> for forward filling (propagating the last valid value forward) and <code>bfill</code> for backward filling (propagating the next valid value backward).</p>
<pre><code class="language-python"># Backward fill

df.bfill()

# Forward fill

df.ffill()
</code></pre>
<p><img src="./images/image2.png" alt="bfill() vs ffill()" /></p>
<h3 id="groupby"><a class="header" href="#groupby">GroupBy</a></h3>
<p>Pandas groupby is a method that splits the dataframe into groups based on one or more columns, applies a function to each group, and combines the results into a new DataFrame.</p>
<p>Example:</p>
<pre><code class="language-python">Grouped = df.groupby('Category')

Result = Grouped.agg({'Value': ['mean', 'sum', 'count', 'max', 'min']})
</code></pre>
<p><img src="./images/image3.png" alt="GroupBy" /></p>
<hr />
<h2 id="matplotlib"><a class="header" href="#matplotlib">Matplotlib</a></h2>
<p><img src="./images/image38.png" alt="Matplotlib" /></p>
<p>Matplotlib is a built-in module in Python used for plotting.</p>
<p>Import statement:</p>
<pre><code class="language-python">import matplotlib.pyplot

# or

import matplotlib.pyplot as plt
</code></pre>
<h3 id="line-plot"><a class="header" href="#line-plot">Line Plot</a></h3>
<p>Plots a line graph. It is commonly used for visualizing <code>continuous data</code>, like time
series or continuous functions.</p>
<pre><code class="language-python">matplotlib.pyplot.plot()

# or

plt.plot()
</code></pre>
<p><img src="./images/image4.png" alt="Line Plot" /></p>
<h3 id="scatter-plot"><a class="header" href="#scatter-plot">Scatter Plot</a></h3>
<p>Scatter plots are used to visualize the relationship between two numerical variables, allowing you to <code>identify patterns, trends, clusters, correlations, and outliers</code>.</p>
<pre><code class="language-python">matplotlib.pyplot.scatter()

# or

plt.scatter()
</code></pre>
<p><img src="./images/image5.png" alt="Scatter Plot" /></p>
<h3 id="bar-plot"><a class="header" href="#bar-plot">Bar Plot</a></h3>
<p>Bar plots are used to compare the values of different categories, display frequencies or counts of categorical variables, and visualize the relationship between categorical and numerical variables. <code>Useful for comparing discrete data</code>.</p>
<pre><code class="language-python">matplotlib.pyplot.bar()

# or

plt.bar()
</code></pre>
<p><img src="./images/image6.png" alt="Bar Plot" /></p>
<hr />
<h2 id="data-processing"><a class="header" href="#data-processing">Data Processing</a></h2>
<p>If your dataset is based on real-life data, it might not be perfect.</p>
<p>Your dataset might include:</p>
<ul>
<li>Missing values</li>
<li>Erroneous measurements</li>
<li>Noise</li>
</ul>
<h3 id="missing-values"><a class="header" href="#missing-values">Missing Values</a></h3>
<h4 id="how-to-find-missing-values-in-a-pandas-dataframe"><a class="header" href="#how-to-find-missing-values-in-a-pandas-dataframe">How to Find Missing Values in a Pandas DataFrame</a></h4>
<ul>
<li>Check the data type for each column using <code>df.dtypes</code>. If a column has invalid data points, such as empty strings or non-numeric values, the data type will be object.</li>
<li>You can either manually change the data type for all the columns using <code>df.astype()</code> or replace the invalid points with NaN using <code>df.replace()</code>.</li>
<li>Once all the columns are the proper data type, you can count the number of NaN values using one of these methods:</li>
</ul>
<p>Other usefull functions are:</p>
<pre><code class="language-python">df.isnull().sum()

# or

df.isna().sum()

# or

df.info()
</code></pre>
<h4 id="what-to-do-with-missing-values"><a class="header" href="#what-to-do-with-missing-values">What to Do with Missing Values?</a></h4>
<p>Pandas offers several built-in functions to deal with missing values in different ways.</p>
<ul>
<li>You can choose to remove the rows or columns that contain NaN values.</li>
<li>Yu can replace them with a specific value or a calculated value based on the rest of the data.</li>
</ul>
<p><img src="./images/image13.png" alt="Missing Values" /></p>
<h4 id="dropping-nan-values"><a class="header" href="#dropping-nan-values">Dropping NaN Values</a></h4>
<p>Dropping values is easy with a Series, as you can drop the values individually. For DataFrame, it is a bit more complicated as you can not have an uneven number of rows.</p>
<ul>
<li>You can drop any row or drop any column that has at least one NaN value (based on the specified axis).</li>
<li>You can use the <code>how</code> or <code>thresh</code> keywords to specify the number of NaN values that must exist before you drop the row or column.</li>
</ul>
<h4 id="filling-nan-values"><a class="header" href="#filling-nan-values">Filling NaN Values</a></h4>
<ul>
<li><strong>Forward-fill</strong>: Use the previous valid value to fill the missing value, which
can be useful for time series data.</li>
</ul>
<pre><code class="language-python">df.ffill()
</code></pre>
<p><img src="./images/image10.png" alt="ffill()" /></p>
<ul>
<li><strong>Back-fill**</strong>: Use the next valid value to fill the missing value, which can be
useful for reverse time series data.</li>
</ul>
<pre><code class="language-python">df.bfill()
</code></pre>
<p><img src="./images/image11.png" alt="bfill()" /></p>
<ul>
<li><strong>Custom code</strong>: Write your own logic to fill the missing values, which can
be useful for complex or specific cases.</li>
</ul>
<pre><code class="language-python">df.interpolate(method='linear')
</code></pre>
<p><img src="./images/image12.png" alt="interpolate()" /></p>
<h3 id="errors-and-noise"><a class="header" href="#errors-and-noise">Errors and Noise</a></h3>
<h4 id="detecting-errors-in-real-measurements"><a class="header" href="#detecting-errors-in-real-measurements">Detecting Errors in Real Measurements</a></h4>
<ul>
<li>To identify potential outliers, you can use different methods depending on the data's characteristics and shape, such as visualizing or analyzing them statistically.</li>
<li>After finding the errors, you can handle them in the same way as you handled the NaN values. A simple way to code this is to change all the incorrect values to <code>np.nan</code> and then use your preferred method to replace the missing values.</li>
</ul>
<hr />
<h2 id="model-validation"><a class="header" href="#model-validation">Model Validation</a></h2>
<h3 id="training-and-test-sets"><a class="header" href="#training-and-test-sets">Training and Test Sets</a></h3>
<ul>
<li>
<p><strong>Training Set</strong>: The training set is the largest part of the dataset and the foundation for model building. Machine learning algorithms use this segment to learn from the data’s patterns.</p>
</li>
<li>
<p><strong>Test Set</strong>: Different from the training set, the test set serves as an unbiased measure for evaluating the model’s performance on completely new and unseen data.</p>
</li>
</ul>
<p>Example:</p>
<pre><code class="language-python">from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
</code></pre>
<p><img src="./images/image14.png" alt="Train and test" /></p>
<h3 id="cross-validation"><a class="header" href="#cross-validation">Cross-Validation</a></h3>
<p>Cross-validation is a technique to evaluate the performance of a machine learning model on <code>unseen data</code>.</p>
<p>Example:</p>
<p>If 𝒌 = 𝟑, then the data set <code>{𝑥1, 𝑥2, 𝑥3, 𝑥4, 𝑥5, 𝑥6}</code> is divided into
three subsets:</p>
<ul>
<li><code>{𝑥1, 𝑥2}</code></li>
<li><code>{𝑥3, 𝑥4}</code></li>
<li><code>{𝑥5, 𝑥6}</code></li>
</ul>
<pre><code class="language-python">from sklearn.cross_validation import cross_val_score

cross_val_score(model, X, y, cv=3)
</code></pre>
<p><img src="./images/image15.png" alt="Cross-Validation" /></p>
<p>Compute the mean (average) test error across the three folds.</p>
<hr />
<h2 id="model-selection"><a class="header" href="#model-selection">Model Selection</a></h2>
<h3 id="bias-and-variance"><a class="header" href="#bias-and-variance">Bias and Variance</a></h3>
<ul>
<li>
<p><strong>Bias</strong>: The difference between the model's predicted value and the actual value. <code>High bias</code> model tend to <code>underfit</code> the data, meaning they cannot capture the complexity or patterns in the data.</p>
</li>
<li>
<p><strong>Variance</strong>: The sensitivity of the model to changes in the training data. <code>High variance</code> models tend to <code>overfit</code> the data, meaning they cannot generalize well to new or unseen data.</p>
</li>
</ul>
<p><img src="./images/image16.png" alt="Bias and Variance" /></p>
<hr />
<h2 id="supervised-learning"><a class="header" href="#supervised-learning">Supervised Learning</a></h2>
<p>Supervised learning is a type of machine learning that learns labeled data, which consists of input/output pairs. The input can be either a <code>numerical value (regression)</code> or a <code>class (classification)</code>. Supervised learning aims to make accurate predictions for new data that has not been seen before.</p>
<h3 id="regression-and-classification-models"><a class="header" href="#regression-and-classification-models">Regression and Classification Models</a></h3>
<p>Classification and regression are two types of supervised machine learning
problems, where the goal is to learn a mapping function from input variables
to output variables.</p>
<ul>
<li>
<p>In <strong>classification</strong>, we want to assign a discrete label to an input, such as
&quot;spam&quot; or &quot;not spam&quot; for an email.</p>
</li>
<li>
<p>In <strong>regression</strong>, we want to estimate a continuous value for an input, such as
the price of a house based on its features.</p>
</li>
</ul>
<hr />
<h2 id="linear-models"><a class="header" href="#linear-models">Linear Models</a></h2>
<p>Linear models are supervised learning algorithms that predict an output variable based on a <code>linear combination of input features</code>. They can be used for both regression and classification tasks, depending on whether the output variable is continuous or binary.</p>
<hr />
<h2 id="linear-models-for-regression"><a class="header" href="#linear-models-for-regression">Linear Models For Regression</a></h2>
<p>For regression, the general prediction formula for a linear model looks like this:</p>
<p><img src="./images/image26.png" alt="Linear Model" /></p>
<p>Where:</p>
<ul>
<li>
<p><code>y^</code> is the predicted value.</p>
</li>
<li>
<p><code>b</code> is the bias term.</p>
</li>
<li>
<p><code>w</code> is the weight vector.</p>
</li>
<li>
<p><code>x</code> is the input feature vector.</p>
</li>
</ul>
<p>Popular linear models used for regression include:</p>
<ul>
<li>
<p><strong><em>Linear Regression</em></strong></p>
</li>
<li>
<p><strong><em>Ridge Regression</em></strong></p>
</li>
<li>
<p><strong><em>Lasso Regression</em></strong></p>
</li>
</ul>
<h3 id="linear-regression"><a class="header" href="#linear-regression">Linear Regression</a></h3>
<ul>
<li>
<p>Also known as <code>Ordinary Least Squares (OLS)</code>.</p>
</li>
<li>
<p>This is the simplest linear method for regression.</p>
</li>
<li>
<p>Linear regression finds the parameters <code>w</code> and <code>b</code> the mean squared error between predictions, <code>y^</code>, and the true values, <code>y</code>, for the training set.</p>
</li>
<li>
<p>The <code>mean squared error</code> is the sum of the squared differences between the
predictions and the true values, divided by the number of samples.</p>
</li>
</ul>
<p><img src="./images/image27.png" alt="Mean Squared Error" /></p>
<p>Where:</p>
<ul>
<li>
<p><code>N</code> is the number of samples.</p>
</li>
<li>
<p><code>y^[i] = w[0] * x[i][0] + b</code></p>
</li>
<li>
<p><code>y[i]</code> is the true value.</p>
</li>
</ul>
<p>Example:</p>
<pre><code class="language-python">from sklearn.linear_model import LinearRegression

# Instantiate the regressor
lr = LinearRegression()

# Fit the regressor to the data
lr.fit(X, y)

# Predict the labels of the test set
y_pred = lr.predict(X_test)
</code></pre>
<p>The code snippet above shows how to use a <code>linear regression model</code> to predict the labels of the test set.</p>
<p><img src="./images/image28.png" alt="Linear Regression" /></p>
<h3 id="ridge-regression"><a class="header" href="#ridge-regression">Ridge Regression</a></h3>
<ul>
<li>
<p>It is also a linear model for regression, so it uses the same formula as linear regression.</p>
</li>
<li>
<p>For ridge regression, the coefficients <code>w</code> are chosen not only so that they predict well on the training data, but also to fit an additional constraint.</p>
</li>
<li>
<p>The additional constraint is that the magnitude of the coefficients must be as small as possible; all entries of <code>w</code> should be close to zero, called <code>L2 regularization</code>.</p>
</li>
<li>
<p>The square of the <code>L2 norm</code> of the <code>w</code> is defined as:</p>
</li>
</ul>
<p><img src="./images/image29.png" alt="L2 Norm" /></p>
<p>Where:</p>
<ul>
<li>
<p><code>α</code> is a hyperparameter that controls the amount of regularization applied to the coefficients of a linear model. The larger the value, the more aggressive the penalization is. It can be any real value between o and infinity.</p>
</li>
<li>
<p>Regularization means explicitly restricting a model to avoid over-fitting.</p>
</li>
</ul>
<p>Example:</p>
<pre><code class="language-python">from sklearn.linear_model import Ridge

# Instantiate the regressor
ridge = Ridge(alpha=0.1, normalize=True)

# Fit the regressor to the data
ridge.fit(X_train, y_train)

# Predict the labels of the test set
y_pred = ridge.predict(X_test)
</code></pre>
<p>The code snippet above shows how to use a <code>ridge regression model</code> to predict the labels of the test set.</p>
<p><img src="./images/image30.png" alt="Ridge Regression" /></p>
<h3 id="lasso-regression"><a class="header" href="#lasso-regression">Lasso Regression</a></h3>
<ul>
<li>
<p>Alternative to ridge regression for regularizing linear regression.</p>
</li>
<li>
<p>The lasso regression restricts the coefficients to be close to zero, but in a slightly different way, called <code>L1 regularization</code>.</p>
</li>
<li>
<p>The <code>L1 norm</code> of the <code>w</code> is defined as:</p>
</li>
</ul>
<p><img src="./images/image31.png" alt="L1 Norm" /></p>
<p>Where:</p>
<ul>
<li>
<p><code>α</code> is the regularization parameter.</p>
</li>
<li>
<p>The consequence of L1 regularization is that when using the lasso, some
coefficients are exactly zero.</p>
</li>
<li>
<p>This means some features are entirely ignored by the model.</p>
</li>
<li>
<p>This can be seen as a form of automatic feature selection.</p>
</li>
<li>
<p>Can reveal the most important features in the model.</p>
</li>
</ul>
<p>Example:</p>
<pre><code class="language-python">from sklearn.linear_model import Lasso

# Instantiate the regressor
lasso = Lasso(alpha=0.1, normalize=True)

# Fit the regressor to the data
lasso.fit(X_train, y_train)

# Predict the labels of the test set
y_pred = lasso.predict(X_test)
</code></pre>
<p>The code snippet above shows how to use a <code>lasso regression model</code> to predict the labels of the test set.</p>
<p><img src="./images/image32.png" alt="Lasso Regression" /></p>
<hr />
<h2 id="linear-regression-accuracy-metrics"><a class="header" href="#linear-regression-accuracy-metrics">Linear Regression Accuracy Metrics</a></h2>
<ul>
<li>
<p><strong><em>Mean Squared Error (MSE)</em></strong></p>
</li>
<li>
<p><strong><em>R² score (Coefficient of Determination)</em></strong></p>
</li>
</ul>
<h3 id="mean-squared-error-mse"><a class="header" href="#mean-squared-error-mse">Mean Squared Error (MSE)</a></h3>
<p>The Mean Squared Error (MSE) is a fundamental metric used to quantify the goodness of fit of a regression model by measuring the average squared difference between the predicted values, <code>y^</code>, and the actual values, <code>y</code>, of the dependent variable.</p>
<p><img src="./images/image35.png" alt="Mean Squared Error" /></p>
<p>Where:</p>
<ul>
<li>
<p><code>N</code> is the number of datapoints in the dataset.</p>
</li>
<li>
<p><code>y[i]</code> signifies the actual value of the dependent variable for the i-th data point.</p>
</li>
<li>
<p><code>y^[i]</code> corresponds to the predicted value of the dependent variable for the i-th data point based on the regression model.</p>
</li>
</ul>
<p>Example:</p>
<pre><code class="language-python">from sklearn.metrics import mean_squared_error

# Compute the mean squared error of the regressor
mse = mean_squared_error(y_test, y_pred)
</code></pre>
<p>The code snippet above shows how to use the <code>mean squared error</code> to evaluate the performance of a regression model.</p>
<h3 id="r²-score-coefficient-of-determination"><a class="header" href="#r²-score-coefficient-of-determination">R² Score (Coefficient of Determination)</a></h3>
<ol>
<li>R-squared is the ration of the explained variation to the total variation in the dependent variable.</li>
</ol>
<p><img src="./images/image33.png" alt="R² Score" /></p>
<p>Where:</p>
<ul>
<li>
<p><code>RSS</code> is the residual sum of squares.</p>
</li>
<li>
<p><code>TSS</code> is the total sum of squares.</p>
</li>
</ul>
<ol start="2">
<li>R-squared can also be expressed as the square of the correlation coefficient, which measures the strength and direction of the linear relationship between two variables.</li>
</ol>
<p><img src="./images/image34.png" alt="R² Score" /></p>
<p>Where:</p>
<ul>
<li><code>r</code> is the correlation coefficient.</li>
</ul>
<ol start="3">
<li>R-squared can be negative if the model fits worse than a horizontal line, which is the simplest model that uses the mean of the dependent variable as a constant prediction.</li>
</ol>
<p>Example:</p>
<pre><code class="language-python">from sklearn.metrics import r2_score

# Compute the R² score of the regressor
r2 = r2_score(y_test, y_pred)
</code></pre>
<p>The code snippet above shows how to use the <code>R² score</code> to evaluate the performance of a regression model.</p>
<hr />
<h2 id="linear-models-for-classification"><a class="header" href="#linear-models-for-classification">Linear Models For Classification</a></h2>
<p>For classification, the general prediction formula for a linear model looks like this:</p>
<p><img src="./images/image39.png" alt="Linear Model" /></p>
<p>Where:</p>
<ul>
<li>
<p><code>x</code> is the input feature vector.</p>
</li>
<li>
<p><code>w</code> is the weight vector.</p>
</li>
<li>
<p><code>b</code> is the bias term.</p>
</li>
<li>
<p><code>p(x)</code> is the probability that the input <code>x</code> belongs to the positive class.</p>
</li>
</ul>
<p>Some popular linear models used for classification include:</p>
<ul>
<li>
<p><strong><em>Logistic Regression</em></strong></p>
</li>
<li>
<p><strong><em>Support Vector Machines (SVM)</em></strong></p>
</li>
</ul>
<h3 id="logistic-regression"><a class="header" href="#logistic-regression">Logistic Regression</a></h3>
<ul>
<li>
<p>Primarily used for <strong>classification</strong>, not regression, despite its name.</p>
</li>
<li>
<p>It’s a statistical model used to predict the probability of a binary outcome, typically denoted as class 0 and class 1.</p>
</li>
<li>
<p>The logistic regression model estimates the probability that a given input
belongs to one of these two classes.</p>
</li>
</ul>
<p><img src="./images/image40.png" alt="Logistic Regression" /></p>
<p>Example:</p>
<pre><code class="language-python">from sklearn.linear_model import LogisticRegression

# Instantiate the classifier
logreg = LogisticRegression()

# Fit the classifier to the data
logreg.fit(X_train, y_train)

# Predict the labels of the test set
y_pred = logreg.predict(X_test)
</code></pre>
<p>The code snippet above shows how to use a <code>logistic regression model</code> to predict the labels of the test set.</p>
<p><img src="./images/image41.png" alt="Logistic Regression" /></p>
<hr />
<h2 id="linear-classification-accuracy-metrics"><a class="header" href="#linear-classification-accuracy-metrics">Linear Classification Accuracy Metrics</a></h2>
<ul>
<li>
<p><strong><em>Confusion Matrix</em></strong></p>
</li>
<li>
<p><strong><em>Accuracy</em></strong></p>
</li>
<li>
<p><strong><em>Precision</em></strong></p>
</li>
<li>
<p><strong><em>Recall</em></strong></p>
</li>
<li>
<p><strong><em>F1 Score</em></strong></p>
</li>
</ul>
<h3 id="confusion-matrix"><a class="header" href="#confusion-matrix">Confusion Matrix</a></h3>
<p>A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known.</p>
<ul>
<li><strong>True Positives (TP)</strong>: The number of instances that are actually positive (P) and are correctly predicted as positive by the classification algorithm.</li>
<li><strong>False Positives (FP)</strong>: The number of instances that are actually negative (N) but are incorrectly predicted as positive (P) by the algorithm.</li>
<li><strong>True Negatives (TN)</strong>: The number of instances that are actually negative (N) and are correctly predicted as negative by the algorithm.</li>
<li><strong>False Negatives (FN)</strong>: The number of instances that are actually positive (P) but are incorrectly predicted as negative (N) by the algorithm.</li>
</ul>
<p><img src="./images/image17.png" alt="Confusion Matrix" /></p>
<p>Example:</p>
<pre><code class="language-python">from sklearn.metrics import confusion_matrix

# Compute the confusion matrix of the classifier
cm = confusion_matrix(y_test, y_pred)
</code></pre>
<p>The code snippet above shows how to use a <code>confusion matrix</code> to evaluate the performance of a classification model.</p>
<h3 id="accuracy"><a class="header" href="#accuracy">Accuracy</a></h3>
<p>Accuracy is a metric that quantifies the ratio of correctly classified instances to
the total predictions made by a model.</p>
<p><img src="./images/image18.png" alt="Accuracy" /></p>
<p>Example:</p>
<pre><code class="language-python">from sklearn.metrics import accuracy_score

# Compute the accuracy of the classifier
accuracy = accuracy_score(y_test, y_pred)
</code></pre>
<p>The code snippet above shows how to use the <code>accuracy</code> metric to evaluate the performance of a classification model.</p>
<h3 id="precision"><a class="header" href="#precision">Precision</a></h3>
<p>Precision is a metric that measures the accuracy of <code>positive predictions</code> generated
by a model, taking <code>false positives</code> into account.</p>
<p><img src="./images/image19.png" alt="Precision" /></p>
<p>Example:</p>
<pre><code class="language-python">from sklearn.metrics import precision_score

# Compute the precision of the classifier
precision = precision_score(y_test, y_pred)
</code></pre>
<p>The code snippet above shows how to use the <code>precision</code> metric to evaluate the performance of a classification model.</p>
<h3 id="recall"><a class="header" href="#recall">Recall</a></h3>
<p>Recall, also known as sensitivity or the true positive rate, quantifies a model’s
capacity to identify all positive instances, even when considering false negatives.</p>
<p><img src="./images/image20.png" alt="Recall" /></p>
<p>Example:</p>
<pre><code class="language-python">from sklearn.metrics import recall_score

# Compute the recall of the classifier
recall = recall_score(y_test, y_pred)
</code></pre>
<p>The code snippet above shows how to use the <code>recall</code> metric to evaluate the performance of a classification model.</p>
<h3 id="f1-score"><a class="header" href="#f1-score">F1 Score</a></h3>
<p>The F1-Score presents a harmonious equilibrium between precision and recall,
while accounting for both false positives and false negatives.</p>
<p><img src="./images/image21.png" alt="F1 Score" /></p>
<p>Example:</p>
<pre><code class="language-python">from sklearn.metrics import f1_score

# Compute the F1 score of the classifier
f1 = f1_score(y_test, y_pred)
</code></pre>
<p>The code snippet above shows how to use the <code>F1 score</code> to evaluate the performance of a classification model.</p>
<hr />
<h2 id="decision-trees"><a class="header" href="#decision-trees">Decision Trees</a></h2>
<p><img src="./images/image22.png" alt="Decision Trees" /></p>
<p>Decision trees are widely used models for classification and regression tasks. They learn a hierarchy of if/else questions, leading to a decision.</p>
<h3 id="key-concepts"><a class="header" href="#key-concepts">Key Concepts</a></h3>
<ul>
<li>
<p>The top of the decision tree is referred to as the root node.</p>
</li>
<li>
<p>A leaf node is a node that has no children. A node that does have children is known as an internal node.</p>
</li>
<li>
<p>Nodes in a tree are leveled by their distance from the root (level 0). The tree’s height is the maximum level of any node.</p>
</li>
</ul>
<h3 id="gini-impurity"><a class="header" href="#gini-impurity">Gini Impurity</a></h3>
<p>The Gini impurity is a measure of how likely a randomly chosen element from a set would be incorreclty labeled if it was randomly labeled according to the distribution of labels in the set.</p>
<p><img src="./images/image23.png" alt="Gini Impurity" /></p>
<p>Where <strong><em>k</em></strong> is the number of classes in <strong><em>pi</em></strong> is the probability of choosing an element of class <strong><em>i</em></strong>. The Gini impurity ranges from <code>0</code> to <code>0.5</code>.</p>
<ul>
<li>
<p><code>0</code> means the set is <strong>perfectly pure</strong> (all the elements belong to the same class).</p>
</li>
<li>
<p><code>0.5</code> means the set is <strong>completely impure</strong> (equal probability of choosing any class).</p>
</li>
</ul>
<h3 id="decision-tree-classification-uses-gini-impurity"><a class="header" href="#decision-tree-classification-uses-gini-impurity">Decision Tree Classification Uses Gini Impurity</a></h3>
<ul>
<li>
<p>To use Gini in decision tree classification, the algorithm compares the Gini values of different possible splits and chooses the one that minimizes the Gini value.</p>
</li>
<li>
<p>This means that the algorithm tries to find the best feature and the best threshold to divide the data into two subsets, such that the subsets are more pure than the original node.</p>
</li>
<li>
<p>The algorithm repeats this process recursively until all the nodes are pure or some stopping criteria are met.</p>
</li>
</ul>
<h3 id="predicting-new-values"><a class="header" href="#predicting-new-values">Predicting New Values</a></h3>
<ul>
<li>
<p>A prediction on a new data point is made by checking which region of the partition the point lies in and then assigning the majority target (or the single target in the case of pure leaves) in that region to the predicted value.</p>
</li>
<li>
<p>The region can be found by traversing the tree from the root and going left or right, depending on whether the test is fulfilled or not.</p>
</li>
</ul>
<pre><code class="language-python">from sklearn.tree import DecisionTreeClassifier

dtc = DecisionTreeClassifier(max_depth=2, random_state=0)

# Fit the classifier to the data
dtc.fit(X_train, y_train)

# Predict the labels of the test set
y_pred = dtc.predict(X_test)
</code></pre>
<p>The code snippet above shows how to use a <code>decision tree classifier</code> to predict the labels of the test set.</p>
<h3 id="difference-between-hyperparameter-and-parameter"><a class="header" href="#difference-between-hyperparameter-and-parameter">Difference Between Hyperparameter and Parameter</a></h3>
<ul>
<li>
<p><strong>Hyperparameter</strong>: It’s a configuration setting for the model. Its value is set prior to the commencement of the learning process and is not learned from the data.</p>
</li>
<li>
<p><strong>Parameters</strong>: It’s an internal variable of a model. Its value is learned from the
data during the training process.</p>
</li>
</ul>
<h3 id="decision-tree-hyperparameter"><a class="header" href="#decision-tree-hyperparameter">Decision Tree Hyperparameter</a></h3>
<ul>
<li>
<p><strong>max_depth</strong>: This hyperparameter controls the maximum depth of the three.</p>
</li>
<li>
<p><strong>min_samples_split</strong>: This hyperparameter dictates the minimum number of sample required to split an internal node. By increasing this value, the tree becomes more constrained as it has to consider more samples at each node, making it harder for the model to fit to noise in the training data.</p>
</li>
<li>
<p><strong>min_samples_leaf</strong>: This is the minimum number of samples required to be at a leaf node. This hyperparameter prevents the model from learning very specific patterns from the training data.</p>
</li>
<li>
<p><strong>max_features</strong>: The number of features to consider when looking for the best split. By reducing the number of features considered at each split, we can add randomness to the model making it more robust to noise.</p>
</li>
</ul>
<p>Example:</p>
<pre><code class="language-python">from sklearn.tree import DecisionTreeClassifier

dtc = DecisionTreeClassifier(max_depth=None, max_leaf_nodes=5, random_state=0)

# Fit the classifier to the data
y_pred = dtc.fit(X_train, y_train)

# Predict the labels of the test set
y_pred = dtc.predict(X_test)
</code></pre>
<p>The code snippet above shows how to use a <code>decision tree classifier</code> to predict the labels of the test set.</p>
<p><img src="./images/image24.png" alt="Decision Tree" /></p>
<h3 id="decision-tree-regression"><a class="header" href="#decision-tree-regression">Decision Tree Regression</a></h3>
<ul>
<li>
<p>Decision trees can also be used for regression.</p>
</li>
<li>
<p>Splits are evaluated based on Mean Squared Error (MSE) instead of Gini impurity.</p>
</li>
<li>
<p>Subsequent levels result in reduced mean squared error.</p>
</li>
</ul>
<pre><code class="language-python">from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Instantiate the regressor
dtr = DecisionTreeRegressor(criterion='squared_error', splitter='best', max_leaf_nodes= 2)

# Fit the regressor to the data
_ = dtr.fit(X, y)

# Predict the labels of the test set
y_hat = dtr.predict(X[X[:, 1] &lt;= 1.067])

# Compute the mean squared error of the regressor
mse = metrics.mean_squared_error(y_hat, y[X[:, 1] &lt;= 1.067])
</code></pre>
<p>The code snippet above shows how to use a <code>decision tree regressor</code> to predict the labels of the test set and compute the <code>mean squared error</code> of the regressor.</p>
<h3 id="decision-tree-strengths"><a class="header" href="#decision-tree-strengths">Decision Tree Strengths</a></h3>
<ul>
<li>
<p>As each feature is processed separately, no pre-processing like normalization or standardization of features is needed.</p>
</li>
<li>
<p>Decision trees work well when you have features that are on
completely different scales, or a mix of binary and continuous
features.</p>
</li>
<li>
<p>The resulting model can easily be visualized and understood by
non-experts (at least for smaller trees).</p>
</li>
</ul>
<h3 id="decision-tree-weakness"><a class="header" href="#decision-tree-weakness">Decision Tree Weakness</a></h3>
<ul>
<li>Even with the use of pre-pruning, decision tree models tend to over-fit and provide poor generalization performance.</li>
</ul>
<hr />
<h2 id="random-forest"><a class="header" href="#random-forest">Random Forest</a></h2>
<p><img src="./images/image25.png" alt="Random Forest" /></p>
<p>A Random Forest is essentially a collection of Decision Trees, where every tree is slightly different from the others. The idea behind random forests is that each tree might do a relatively good job of predicting, but might over-fit on part of the data.</p>
<h3 id="steps-to-create-a-random-forest"><a class="header" href="#steps-to-create-a-random-forest">Steps to Create a Random Forest</a></h3>
<ol>
<li>
<p>Select the number of trees to use (hyperparameter is <strong><em>n_estimators</em></strong>).</p>
</li>
<li>
<p>Random forests get their name from injecting randomness into the tree building to ensure each tree is different. There are two ways in which the trees in a random forest are randomized:</p>
</li>
</ol>
<ul>
<li>
<p>By selecting the data points used to build a tree.</p>
<ul>
<li>
<p>For each tree, a bootstrap sample is created.</p>
</li>
<li>
<p>A bootstrap sample is the same size as the original data, but contains a random assortment of the data, where some of the data samples are missing (approx. 1/3) and some data samples are repeated.</p>
</li>
<li>
<p>A decision tree is then made using the bootstrap sample.</p>
</li>
</ul>
</li>
<li>
<p>By selecting the features in each split test.</p>
<ul>
<li>
<p>the algorithm randomly selects a subset of the features, and it looks for the best possible test involving one of these features.</p>
</li>
<li>
<p>The number of features that are selected is controlled by the max_features parameter</p>
</li>
<li>
<p>This selection of a subset of features is repeated separately in each node, so that each node in the tree splits the dataset using a different subset of the features.</p>
</li>
</ul>
</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../../chapters/Infrastructure_As_Code/index.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../../chapters/Software_Architecture/index.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../../chapters/Infrastructure_As_Code/index.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../../chapters/Software_Architecture/index.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../../elasticlunr.min.js"></script>
        <script src="../../mark.min.js"></script>
        <script src="../../searcher.js"></script>

        <script src="../../clipboard.min.js"></script>
        <script src="../../highlight.js"></script>
        <script src="../../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
